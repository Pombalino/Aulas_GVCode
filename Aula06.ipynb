{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 06: WebScraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um dos maiores desafios de Ciência de Dados é conseguir os Dados. \n",
    "\n",
    "Uma forma é conseguir dados com o cliente/sua empresa e analisá-los.\n",
    "\n",
    "Uma outra forma de conseguir é através de **[APIs](https://codeburst.io/6-interesting-apis-to-check-out-in-2018-5d6830063f29)**, interfaces para se conseguir dados oficialmente de fontes confiáveis. \n",
    "\n",
    "O **Facebook** tinha uma ótima API. Tinha. Depois do escândalo da Cambridge Analytica, boa parte das funções interessantes foi tirada.\n",
    "\n",
    "O **[Instagram](https://www.instagram.com/developer/)**, **[Twitter](https://developer.twitter.com/en/docs.html)** e a **[NASA](https://api.nasa.gov/)** tem ótimas APIs para diversas aplicações, recomendo ir atrás delas.\n",
    "\n",
    "Quando isso não é possível (a fonte não tem uma API, ou os dados estão **desestruturados** e em *sites*, existe o **WebScraping**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Os algoritmos de WebScraping fazem um *request*, pegam o código-fonte da página e buscam a informação que você pedir nele\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos buscar informações sobre o histórico de uso de um site, através do site [Rank2Traffic](https://www.rank2traffic.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.rank2traffic.com/stackoverflow.com' # Because Stack Overflow is God."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = req.Session() # Começando a sessão, como se estivesse abrindo o browser\n",
    "# Os headers fazem o Python fingir ser um browser comum, para evitar que o site bloqueie ele\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5)AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,/;q=0.8\"}\n",
    "ref = session.get(url, headers=headers) # Aqui ele abre o site e puxa o código-fonte\n",
    "soup = bs(ref.text, \"html.parser\") # E transforma o código em texto (método .text) e logo depois em um objeto BeautifulSoup\n",
    "print(soup) # que tem essa cara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraindo a tabela\n",
    "tab = soup.find('table', {'id':\"myTab1-table\"}) # método .find: encontre a primeira tabela com id 'myTab1-table'\n",
    "rows=list() # para salvar as linhas da tabela\n",
    "for row in tab.findAll(\"tr\"):\n",
    "    try:\n",
    "        rows.append(row.findAll(\"td\")[1].string.replace(\" \",\"\").replace(\"\\n\",\"\"))# a segunda coluna (índice 1) é a que interessa\n",
    "    except IndexError: # na primeira linha, não tem índice 1. então coloque o nome do site \n",
    "        rows.append(url[-17:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraindo dados complementares de uso\n",
    "meta = soup.findAll('div',{'class':'infobox-data'})[1:] # método .findAll: Encontre TODOS nessas condições, e retorne uma lista\n",
    "for box in meta:\n",
    "    rows.append(box.find('span').string)\n",
    "    rows.append(box.find('div').string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraindo percentual de acessos por país\n",
    "tab = soup.find('table', {'class':\"table table-hover table-condensed\"})\n",
    "try: \n",
    "    for row in tab.findAll(\"tr\"):\n",
    "        try:\n",
    "            rows.append(row.findAll(\"td\")[0].string)\n",
    "            rows.append(row.findAll(\"td\")[1].string)\n",
    "        except IndexError:\n",
    "            rows.append('Paises')\n",
    "except AttributeError:\n",
    "    rows.append('Supondo, sem info oficial')\n",
    "    rows.append('100%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_excel(\"StackOverflow.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
